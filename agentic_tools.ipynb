{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1019a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.workflow import Context\n",
    "import asyncio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21f6359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "Settings.llm = Ollama(\n",
    "    model=\"llama3.2:latest\",\n",
    "    request_timeout=360.0,\n",
    "    # Manually set the context window to limit memory usage\n",
    "    context_window=8000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e363508",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d92b5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart = []\n",
    "\n",
    "# Define a simple calculator tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    print(\"Using multiply tool\")\n",
    "    return a + b\n",
    "\n",
    "async def search_documents(query: str) -> str:\n",
    "    \"\"\"Useful for answering natural language questions about products of a furniture store items.\"\"\"\n",
    "    response = await query_engine.aquery(query)\n",
    "    return str(response)\n",
    "\n",
    "def addToCart(item)->str:\n",
    "    \"\"\"Useful for adding an item to the cart\"\"\"\n",
    "    cart.append(item)\n",
    "    return \"Item added to the cart\"\n",
    "\n",
    "# Create an agent workflow with our calculator tool\n",
    "agent = FunctionAgent(\n",
    "    tools=[multiply,addToCart,search_documents],\n",
    "    llm=Ollama(model=\"llama3.2:latest\",thinking=False),\n",
    "    system_prompt=\"\"\"You are a helpful assistant that can : \n",
    "    1. multiply two numbers,\n",
    "    2. Add an item to the cart.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59bde583",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = Context(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ebe2e14e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (15.9 GiB) than is available (12.6 GiB) (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m respnse = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(\u001b[33m\"\u001b[39m\u001b[33mAdd trimmer to the cart\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/aumoza/Strg_1/Freelance/personal/Chatbot/chatbot/lib/python3.13/site-packages/workflows/workflow.py:439\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m(ctx)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    437\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    443\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/aumoza/Strg_1/Freelance/personal/Chatbot/chatbot/lib/python3.13/site-packages/workflows/context/context.py:822\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, verbose, run_id, worker_id, resource_manager)\u001b[39m\n\u001b[32m    813\u001b[39m \u001b[38;5;28mself\u001b[39m.write_event_to_stream(\n\u001b[32m    814\u001b[39m     StepStateChanged(\n\u001b[32m    815\u001b[39m         name=name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    819\u001b[39m     )\n\u001b[32m    820\u001b[39m )\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    823\u001b[39m     kwargs.clear()\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit the retrying loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/aumoza/Strg_1/Freelance/personal/Chatbot/chatbot/lib/python3.13/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/aumoza/Strg_1/Freelance/personal/Chatbot/chatbot/lib/python3.13/site-packages/llama_index/core/agent/workflow/base_agent.py:390\u001b[39m, in \u001b[36mBaseWorkflowAgent.run_agent_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    387\u001b[39m user_msg_str = \u001b[38;5;28;01mawait\u001b[39;00m ctx.store.get(\u001b[33m\"\u001b[39m\u001b[33muser_msg_str\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    388\u001b[39m tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tools(user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m agent_output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.take_step(\n\u001b[32m    391\u001b[39m     ctx,\n\u001b[32m    392\u001b[39m     ev.input,\n\u001b[32m    393\u001b[39m     tools,\n\u001b[32m    394\u001b[39m     memory,\n\u001b[32m    395\u001b[39m )\n\u001b[32m    397\u001b[39m ctx.write_event_to_stream(agent_output)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m agent_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/aumoza/Strg_1/Freelance/personal/Chatbot/chatbot/lib/python3.13/site-packages/llama_index/core/agent/workflow/function_agent.py:120\u001b[39m, in \u001b[36mFunctionAgent.take_step\u001b[39m\u001b[34m(self, ctx, llm_input, tools, memory)\u001b[39m\n\u001b[32m    115\u001b[39m ctx.write_event_to_stream(\n\u001b[32m    116\u001b[39m     AgentInput(\u001b[38;5;28minput\u001b[39m=current_llm_input, current_agent_name=\u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m    117\u001b[39m )\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.streaming:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     last_chat_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_streaming_response(\n\u001b[32m    121\u001b[39m         ctx, current_llm_input, tools\n\u001b[32m    122\u001b[39m     )\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    124\u001b[39m     last_chat_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_response(current_llm_input, tools)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/aumoza/Strg_1/Freelance/personal/Chatbot/chatbot/lib/python3.13/site-packages/llama_index/core/agent/workflow/function_agent.py:75\u001b[39m, in \u001b[36mFunctionAgent._get_streaming_response\u001b[39m\u001b[34m(self, ctx, current_llm_input, tools)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# last_chat_response will be used later, after the loop.\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# We initialize it so it's valid even when 'response' is empty\u001b[39;00m\n\u001b[32m     74\u001b[39m last_chat_response = ChatResponse(message=ChatMessage())\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m     76\u001b[39m     tool_calls = \u001b[38;5;28mself\u001b[39m.llm.get_tool_calls_from_response(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     77\u001b[39m         last_chat_response, error_on_no_tool_call=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     78\u001b[39m     )\n\u001b[32m     79\u001b[39m     raw = (\n\u001b[32m     80\u001b[39m         last_chat_response.raw.model_dump()\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(last_chat_response.raw, BaseModel)\n\u001b[32m     82\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m last_chat_response.raw\n\u001b[32m     83\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/aumoza/Strg_1/Freelance/personal/Chatbot/chatbot/lib/python3.13/site-packages/llama_index/core/llms/callbacks.py:89\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat.<locals>.wrapped_gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     87\u001b[39m last_response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f_return_val:\n\u001b[32m     90\u001b[39m         dispatcher.event(\n\u001b[32m     91\u001b[39m             LLMChatInProgressEvent(\n\u001b[32m     92\u001b[39m                 messages=messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m             )\n\u001b[32m     96\u001b[39m         )\n\u001b[32m     97\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m cast(ChatResponse, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/aumoza/Strg_1/Freelance/personal/Chatbot/chatbot/lib/python3.13/site-packages/llama_index/llms/ollama/base.py:483\u001b[39m, in \u001b[36mOllama.astream_chat.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    480\u001b[39m seen_tool_calls = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    481\u001b[39m all_tool_calls = []\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    485\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/aumoza/Strg_1/Freelance/personal/Chatbot/chatbot/lib/python3.13/site-packages/ollama/_client.py:741\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    740\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m e.response.aread()\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.aiter_lines():\n\u001b[32m    744\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: model requires more system memory (15.9 GiB) than is available (12.6 GiB) (status code: 500)"
     ]
    }
   ],
   "source": [
    "respnse = await agent.run(\"Add trimmer to the cart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47407755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text='Your trimmer has been successfully added to the cart. Let me know if you need anything else! ðŸ˜Š')]), structured_response=None, current_agent_name='Agent', raw={'model': 'qwen3:0.6b', 'created_at': '2025-10-21T10:31:59.354246162Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1144955232, 'load_duration': 54649128, 'prompt_eval_count': 309, 'prompt_eval_duration': 348181798, 'eval_count': 23, 'eval_duration': 726126303, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None), 'usage': {'prompt_tokens': 309, 'completion_tokens': 23, 'total_tokens': 332}}, tool_calls=[ToolCallResult(tool_name='addToCart', tool_kwargs={'item': 'trimmer'}, tool_id='addToCart', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='Item added to the cart')], tool_name='addToCart', raw_input={'args': (), 'kwargs': {'item': 'trimmer'}}, raw_output='Item added to the cart', is_error=False), return_direct=False)], retry_messages=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respnse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84d23562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lipbam', 'trimmer']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e815fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await agent.run(\"My name is Aum\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff397bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text='My name is Aum. ðŸ˜Š')])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "406cb934",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await agent.run(\"What is your name?\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05066da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text='My name is Logan. ðŸ˜Š')])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d166128",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await agent.run(\"What products do you have in your furniture store?\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c71f250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text=\"I don't have a specific product list. However, I can assist you with purchasing items if you have a product in mind. Can you tell me what product you're looking for? ðŸ˜Š\")])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5bec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
